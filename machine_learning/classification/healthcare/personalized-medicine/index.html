<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<title>Personalized Medicine Kaggle Competition</title>
<meta name="og:title" content="Philipp Schwarz"/>
<meta name="og:description" content="Machine Learning Engineer & Data Scientist  with strong statistical  modeling and programming skills. I prefer working in a high-performance environment and in an interdisciplinary team. Reach out to me if you got a project that is unprecedented, and you need a pragmatic guy that delivers."/>
<meta property="og:image" content="/assets/portfolio.png"/>
<meta property="og:url" content="/"/></head>
<body>
	<main class="container">
		<section class="about">
			<a href="/"><img src="/assets/portfolio.png" alt="Philipp Schwarz"></a>
			<h2><a href="/" style="text-decoration: none">Philipp Schwarz</a></h2>
			<p class="tagline">Data Scientist</p>
			<ul class="social"><a href="https://github.com/philippschw"><li><i class="icon-github-circled"></i></li></a><a href="https://www.linkedin.com/in/philippschw"><li><i class="icon-linkedin-squared"></i></li></a><a href="https://twitter.com/philippschw"><li><i class="icon-twitter-squared"></i></li></a></ul>
			<p>&copy; 2020</p>
		</section>
		<section class="content">
			<div class="post-container">
  <a class="post-link" href="/machine_learning/classification/healthcare/personalized-medicine/">
    <h2 class="post-title">Personalized Medicine Kaggle Competition</h2>
  </a>
  <div class="post-meta">
    <ul class="post-categories"><li>machine_learning</li><li>classification</li><li>healthcare</li></ul>
    <div class="post-date"><i class="icon-calendar"></i>Oct 7, 2017</div>
  </div>
  <div class="post">
    <p>This notebook describes my approach to the <a href="https://www.kaggle.com/c/msk-redefining-cancer-treatment">Personalized Healthcare Redefining Cancer Treatment Kaggle 
competition</a>. This was a research competition at Kaggle in cooperation with the Memorial Sloan Kettering Cancer Center (MSKCC). The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.</p>

<p>Tumors contain cells with many different abnormal mutations in their DNA: some of these mutations are the drivers of tumor growth, whereas others are neutral and considered <em>passengers</em>. Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually anotated into 9 different categories. The goal is to predict the correct category of mutations in the test set.</p>

<p>The model and submission described here got me to the 140th place (out of 1386 teams) or top 11%.</p>

<h1 id="data">Data</h1>

<p>The data comes in two different kinds of files: one of them contains information about the genetic variants (<em>training_variants</em> and <em>stage2_test_variants.csv</em>) and the other contains the text (clinical evidence) that was used to manually classify the variants (<em>training_text</em> and <em>stage2_test_text.csv</em>). The training data contains a class target feature corresponding to one of the 9 categories that variants can be classified as.</p>

<p><em>Note: the “stage2” prefix of the test files is due to the nature of the competition. There was an initial test set that was used at the beginning of the competition and a “stage2” test set that was used in the final week before the deadline to make the submissions.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_variant</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"input/training_variants"</span><span class="p">)</span>
<span class="n">test_variant</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"input/stage2_test_variants.csv"</span><span class="p">)</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"input/training_text"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">"</span><span class="err">\</span><span class="s">|</span><span class="err">\</span><span class="s">|"</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'python'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">"ID"</span><span class="p">,</span><span class="s">"Text"</span><span class="p">])</span>
<span class="n">test_text</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"input/stage2_test_text.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">"ID"</span><span class="p">,</span> <span class="s">"Text"</span><span class="p">])</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">train_variant</span><span class="p">,</span> <span class="n">train_text</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'ID'</span><span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Class'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Number of training variants: </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_size</span><span class="p">))</span>
<span class="c"># number of train data : 3321</span>

<span class="n">test_x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">test_variant</span><span class="p">,</span> <span class="n">test_text</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'ID'</span><span class="p">)</span>
<span class="n">test_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Number of test variants: </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_size</span><span class="p">))</span>
<span class="c"># number of test data : 5668</span>

<span class="n">test_index</span> <span class="o">=</span> <span class="n">test_x</span><span class="p">[</span><span class="s">'ID'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">all_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">all_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
<span class="n">all_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">"ID"</span><span class="p">,</span> <span class="s">"Gene"</span><span class="p">,</span> <span class="s">"Variation"</span><span class="p">,</span> <span class="s">"Text"</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of training variants: 3321
Number of test variants: 986
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>Gene</th>
      <th>Variation</th>
      <th>Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>FAM58A</td>
      <td>Truncating Mutations</td>
      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>CBL</td>
      <td>W802*</td>
      <td>Abstract Background  Non-small cell lung canc...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>CBL</td>
      <td>Q249E</td>
      <td>Abstract Background  Non-small cell lung canc...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>CBL</td>
      <td>N454D</td>
      <td>Recent evidence has demonstrated that acquired...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>CBL</td>
      <td>L399V</td>
      <td>Oncogenic mutations in the monomeric Casitas B...</td>
    </tr>
  </tbody>
</table>
</div>

<p>The data from the different train and test files is now consolidated into one single file. This is necessary for the correct vectorization of the text data and categorical data later on. We can see that the text information resembles scientific article text. We will process this consolidated file in the next step.</p>

<h1 id="preprocessing">Preprocessing</h1>

<p>In order to be able to use this data to train a machine learning model, we need to extract the features from the dataset. This means that we have to transform the text data into vectors that can be understood by an algorithm. I used a modified version of <a href="https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77">this script published on Kaggle.</a> Afterwards we will have the data in a form that I can use to train a neural network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77</span>

<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">gensim.models.doc2vec</span> <span class="kn">import</span> <span class="n">LabeledSentence</span>
<span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">utils</span>

<span class="k">def</span> <span class="nf">constructLabeledSentences</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">sentences</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
        <span class="n">sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LabeledSentence</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">row</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="p">[</span><span class="s">'Text'</span> <span class="o">+</span> <span class="s">'_</span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">sentences</span>

<span class="k">def</span> <span class="nf">textClean</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r"[^A-Za-z0-9^,!.\/'+-=]"</span><span class="p">,</span> <span class="s">" "</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">stops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">"english"</span><span class="p">))</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">stops</span><span class="p">]</span>    
    <span class="n">text</span> <span class="o">=</span> <span class="s">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">cleanup</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">textClean</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">text</span><span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">maketrans</span><span class="p">(</span><span class="s">""</span><span class="p">,</span><span class="s">""</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">text</span>

<span class="n">allText</span> <span class="o">=</span> <span class="n">all_data</span><span class="p">[</span><span class="s">'Text'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">cleanup</span><span class="p">)</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">constructLabeledSentences</span><span class="p">(</span><span class="n">allText</span><span class="p">)</span>
<span class="n">allText</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using TensorFlow backend.





0    cyclindependent kinases cdks regulate variety ...
1    abstract background nonsmall cell lung cancer ...
2    abstract background nonsmall cell lung cancer ...
3    recent evidence demonstrated acquired uniparen...
4    oncogenic mutations monomeric casitas blineage...
Name: Text, dtype: object
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77</span>

<span class="c"># PROCESS TEXT DATA</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Doc2Vec</span>

<span class="n">Text_INPUT_DIM</span><span class="o">=</span><span class="mi">300</span>

<span class="n">text_model</span><span class="o">=</span><span class="bp">None</span>
<span class="n">filename</span><span class="o">=</span><span class="s">'docEmbeddings_5_clean.d2v'</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">text_model</span> <span class="o">=</span> <span class="n">Doc2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">text_model</span> <span class="o">=</span> <span class="n">Doc2Vec</span><span class="p">(</span><span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Text_INPUT_DIM</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">text_model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
    <span class="n">text_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">text_model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">text_model</span><span class="o">.</span><span class="nb">iter</span><span class="p">)</span>
    <span class="n">text_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

<span class="n">text_train_arrays</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">train_size</span><span class="p">,</span> <span class="n">Text_INPUT_DIM</span><span class="p">))</span>
<span class="n">text_test_arrays</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">test_size</span><span class="p">,</span> <span class="n">Text_INPUT_DIM</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_size</span><span class="p">):</span>
    <span class="n">text_train_arrays</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_model</span><span class="o">.</span><span class="n">docvecs</span><span class="p">[</span><span class="s">'Text_'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>

<span class="n">j</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span><span class="n">train_size</span><span class="o">+</span><span class="n">test_size</span><span class="p">):</span>
    <span class="n">text_test_arrays</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_model</span><span class="o">.</span><span class="n">docvecs</span><span class="p">[</span><span class="s">'Text_'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">j</span><span class="o">=</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span>
    
<span class="k">print</span><span class="p">(</span><span class="n">text_train_arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>

<span class="c"># PROCESS GENE DATA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="n">Gene_INPUT_DIM</span><span class="o">=</span><span class="mi">25</span>

<span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">Gene_INPUT_DIM</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">one_hot_gene</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">all_data</span><span class="p">[</span><span class="s">'Gene'</span><span class="p">])</span>
<span class="n">truncated_one_hot_gene</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">one_hot_gene</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">one_hot_variation</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">all_data</span><span class="p">[</span><span class="s">'Variation'</span><span class="p">])</span>
<span class="n">truncated_one_hot_variation</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">one_hot_variation</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c"># ENCODE THE LABELS FROM INTEGERS TO VECTORS</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">np_utils</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">label_encoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">encoded_y</span> <span class="o">=</span> <span class="n">np_utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">((</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_y</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="n">encoded_y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>We have processed the train labels, as printed above (<code class="highlighter-rouge">encoded_y</code>), into vectors that contain 1 in the index of the category that the sample belongs to, and zeros in all other indexes.</p>

<p>Moreover, the training and test sets are now stacked together to look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_set</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">truncated_one_hot_gene</span><span class="p">[:</span><span class="n">train_size</span><span class="p">],</span><span class="n">truncated_one_hot_variation</span><span class="p">[:</span><span class="n">train_size</span><span class="p">],</span><span class="n">text_train_arrays</span><span class="p">))</span>
<span class="n">test_set</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">truncated_one_hot_gene</span><span class="p">[</span><span class="n">train_size</span><span class="p">:],</span><span class="n">truncated_one_hot_variation</span><span class="p">[</span><span class="n">train_size</span><span class="p">:],</span><span class="n">text_test_arrays</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Training set shape is: '</span><span class="p">,</span> <span class="n">train_set</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c"># (3321, 350)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test set shape is: '</span><span class="p">,</span> <span class="n">test_set</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c"># (986, 350)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Training set example rows:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_set</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
<span class="c"># [ -2.46065582e-23  -5.21548048e-19  -1.95048372e-20  -2.44542833e-22</span>
<span class="c">#  -1.19176742e-22   1.61985461e-25   2.93618862e-25  -6.23860891e-27</span>
<span class="c">#   1.14583929e-28  -1.79996588e-29]</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Test set example rows:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test_set</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
<span class="c"># [  9.74220189e-33  -1.31484613e-27   4.37925347e-27  -9.88109317e-29</span>
<span class="c">#    7.66365772e-27   6.58254980e-26  -3.74901712e-26  -8.97613299e-26</span>
<span class="c">#   -3.75471102e-23  -1.05563623e-21]</span>
</code></pre></div></div>

<p>Our data is now ready to be fed into a machine learning model, in this case, into a neural network in TensorFlow.</p>

<h2 id="training-a-4-layer-neural-network-for-classification">Training a 4-layer neural network for classification</h2>

<p>The next step is to create a neural network on TensorFlow. I am using a fully-connected neural network with 4 layers. For details on how the network is built, you can check my <a href="https://github.com/dariodata/TensorFlow-MNIST/blob/master/TensorFlow-MNIST.ipynb">TensorFlow MNIST notebook</a>. Wherever necessary, I will explains what adaptations were specifically necessary for this challenge.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>I found it useful to add the current timestamp to the name of the files that the code will output. This helped me to uniquely identify the results from each run.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">timestr</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">Y</span><span class="si">%</span><span class="s">m</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H</span><span class="si">%</span><span class="s">M</span><span class="si">%</span><span class="s">S"</span><span class="p">)</span>
<span class="n">dirname</span> <span class="o">=</span> <span class="s">'output/'</span>  <span class="c"># output directory</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s">''</span>
</code></pre></div></div>

<p>I select 20% of the training data to use as a validation set and be able to quantify my variance (watch out for overfitting), as I don’t want to have an algorithm that only works well with this specific training data set that was provided, but one that generalizes as well as possible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># split data into training and validation sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">encoded_y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">.</span><span class="n">T</span>

<span class="c"># transpose test set</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_set</span><span class="o">.</span><span class="n">T</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># view data set shapes</span>
<span class="k">print</span><span class="p">(</span><span class="s">'X_train: '</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'X_val: '</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Y_train: '</span><span class="p">,</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Y_val: '</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'X_test: '</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_train:  (350, 2656)
X_val:  (350, 665)
Y_train:  (9, 2656)
Y_val:  (9, 665)
X_test:  (350, 986)
</code></pre></div></div>

<p>Now I define the functions needed to build the neural network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_placeholders</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
    <span class="s">"""
    Creates the placeholders for the tensorflow session.

    Arguments:
    n_x -- scalar, dimensions of the input
    n_y -- scalar, number of classes (from 0 to 8, so -&gt; 9)

    Returns:
    X -- placeholder for the data input, of shape [n_x, None] and dtype "float"
    Y -- placeholder for the input labels, of shape [n_y, None] and dtype "float"
    """</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'X'</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'Y'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">():</span>
    <span class="s">"""
    Initializes parameters to build a neural network with tensorflow.

    Returns:
    parameters -- a dictionary of tensors containing W and b for every layer
    """</span>

    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W1'</span><span class="p">,</span> <span class="p">[</span><span class="mi">350</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b1'</span><span class="p">,</span> <span class="p">[</span><span class="mi">350</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W2'</span><span class="p">,</span> <span class="p">[</span><span class="mi">350</span><span class="p">,</span> <span class="mi">350</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b2'</span><span class="p">,</span> <span class="p">[</span><span class="mi">350</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W3'</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">350</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b3'</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
    <span class="n">W4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W4'</span><span class="p">,</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b4'</span><span class="p">,</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">"W1"</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s">"b1"</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s">"W2"</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s">"b2"</span><span class="p">:</span> <span class="n">b2</span><span class="p">,</span>
                  <span class="s">"W3"</span><span class="p">:</span> <span class="n">W3</span><span class="p">,</span>
                  <span class="s">"b3"</span><span class="p">:</span> <span class="n">b3</span><span class="p">,</span>
                  <span class="s">"W4"</span><span class="p">:</span> <span class="n">W4</span><span class="p">,</span>
                  <span class="s">"b4"</span><span class="p">:</span> <span class="n">b4</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">):</span>
    <span class="s">"""
    Implements the forward propagation for the model: (LINEAR -&gt; RELU)^3 -&gt; LINEAR -&gt; SOFTMAX

    Arguments:
    X -- input dataset placeholder, of shape (input size, number of examples)
    parameters -- python dictionary containing your parameters "W" and "b" for every layer
                  the shapes are given in initialize_parameters

    Returns:
    Z4 -- the output of the last LINEAR unit (logits)
    """</span>

    <span class="c"># Retrieve the parameters from the dictionary "parameters"</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'W3'</span><span class="p">]</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span>
    <span class="n">W4</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'W4'</span><span class="p">]</span>
    <span class="n">b4</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">'b4'</span><span class="p">]</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c"># Z1 = np.dot(W1, X) + b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>  <span class="c"># A1 = relu(Z1)</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">)</span>  <span class="c"># add dropout</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c"># Z2 = np.dot(W2, a1) + b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>  <span class="c"># A2 = relu(Z2)</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">)</span>  <span class="c"># add dropout</span>
    <span class="n">Z3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>  <span class="c"># Z3 = np.dot(W3,Z2) + b3</span>
    <span class="n">A3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z3</span><span class="p">)</span>
    <span class="n">Z4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W4</span><span class="p">,</span> <span class="n">A3</span><span class="p">)</span> <span class="o">+</span> <span class="n">b4</span>

    <span class="k">return</span> <span class="n">Z4</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">Z4</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="s">"""
    Computes the cost

    Arguments:
    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (n_classes, number of examples)
    Y -- "true" labels vector placeholder, same shape as Z4

    Returns:
    cost - Tensor of the cost function
    """</span>

    <span class="c"># transpose to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Z4</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_mini_batches</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="s">"""
    Creates a list of random minibatches from (X, Y)

    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector, of shape (1, number of examples)
    mini_batch_size - size of the mini-batches, integer
    seed

    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c"># number of training examples</span>
    <span class="n">mini_batches</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c"># Step 1: Shuffle (X, Y)</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>
    <span class="n">shuffled_X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">permutation</span><span class="p">]</span>
    <span class="n">shuffled_Y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">permutation</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">m</span><span class="p">))</span>

    <span class="c"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span>
    <span class="n">num_complete_minibatches</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span>
        <span class="n">m</span> <span class="o">/</span> <span class="n">mini_batch_size</span><span class="p">)</span>  <span class="c"># number of mini batches of size mini_batch_size in your partitioning</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_complete_minibatches</span><span class="p">):</span>
        <span class="n">mini_batch_X</span> <span class="o">=</span> <span class="n">shuffled_X</span><span class="p">[:,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span><span class="p">:</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="o">+</span> <span class="n">mini_batch_size</span><span class="p">]</span>
        <span class="n">mini_batch_Y</span> <span class="o">=</span> <span class="n">shuffled_Y</span><span class="p">[:,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span><span class="p">:</span> <span class="n">k</span> <span class="o">*</span> <span class="n">mini_batch_size</span> <span class="o">+</span> <span class="n">mini_batch_size</span><span class="p">]</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="p">(</span><span class="n">mini_batch_X</span><span class="p">,</span> <span class="n">mini_batch_Y</span><span class="p">)</span>
        <span class="n">mini_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">)</span>

    <span class="c"># Handling the end case (last mini-batch &lt; mini_batch_size)</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">%</span> <span class="n">mini_batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mini_batch_X</span> <span class="o">=</span> <span class="n">shuffled_X</span><span class="p">[:,</span> <span class="n">num_complete_minibatches</span> <span class="o">*</span> <span class="n">mini_batch_size</span><span class="p">:</span> <span class="n">m</span><span class="p">]</span>
        <span class="n">mini_batch_Y</span> <span class="o">=</span> <span class="n">shuffled_Y</span><span class="p">[:,</span> <span class="n">num_complete_minibatches</span> <span class="o">*</span> <span class="n">mini_batch_size</span><span class="p">:</span> <span class="n">m</span><span class="p">]</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="p">(</span><span class="n">mini_batch_X</span><span class="p">,</span> <span class="n">mini_batch_Y</span><span class="p">)</span>
        <span class="n">mini_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mini_batches</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b1"</span><span class="p">])</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">])</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b2"</span><span class="p">])</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W3"</span><span class="p">])</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b3"</span><span class="p">])</span>
    <span class="n">W4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W4"</span><span class="p">])</span>
    <span class="n">b4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b4"</span><span class="p">])</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"W1"</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
              <span class="s">"b1"</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
              <span class="s">"W2"</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
              <span class="s">"b2"</span><span class="p">:</span> <span class="n">b2</span><span class="p">,</span>
              <span class="s">"W3"</span><span class="p">:</span> <span class="n">W3</span><span class="p">,</span>
              <span class="s">"b3"</span><span class="p">:</span> <span class="n">b3</span><span class="p">,</span>
              <span class="s">"W4"</span><span class="p">:</span> <span class="n">W4</span><span class="p">,</span>
              <span class="s">"b4"</span><span class="p">:</span> <span class="n">b4</span><span class="p">}</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">"float"</span><span class="p">,</span> <span class="p">[</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">None</span><span class="p">])</span>
    <span class="n">keep_prob1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'keep_prob1'</span><span class="p">)</span>
    <span class="n">keep_prob2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'keep_prob2'</span><span class="p">)</span>

    <span class="n">z4</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">z4</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># dim=0 because the classes are on that axis</span>
    <span class="c"># p = tf.argmax(z4) # this gives only the predicted class as output</span>

    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">prediction</span>
</code></pre></div></div>

<p>And now I define the model function which is in fact the neural network that we will train afterwards. An important difference with respect to <a href="https://github.com/dariodata/TensorFlow-MNIST/blob/master/TensorFlow-MNIST.ipynb">my previous MNIST example</a> is that I added an additional regularization term to the cost function. I used L2 regularization to penalize the weights in all four layers. The bias was not penalized as this is not necessary. The strictness of this penalty was given by a <code class="highlighter-rouge">beta</code> constant defined at 0.01.</p>

<p>Why use additional regularization? Because this allowed me to decrease the variance, i.e. decrease the difference in performance of the model with the training set compared to the validation set. This produced my best submission in the competition.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
          <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"""
    Implements a four-layer tensorflow neural network: (LINEAR-&gt;RELU)^3-&gt;LINEAR-&gt;SOFTMAX.

    Arguments:
    X_train -- training set, of shape (input size, number of training examples)
    Y_train -- test set, of shape (output size, number of training examples)
    X_test -- training set, of shape (input size, number of training examples)
    Y_test -- test set, of shape (output size, number of test examples)
    learning_rate -- learning rate of the optimization
    num_epochs -- number of epochs of the optimization loop
    minibatch_size -- size of a minibatch
    print_cost -- True to print the cost every 100 epochs

    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """</span>

    <span class="n">ops</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>  <span class="c"># to be able to rerun the model without overwriting tf variables</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c"># to keep consistent results</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c"># to keep consistent results</span>
    <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>  <span class="c"># (n_x: input size, m : number of examples in the train set)</span>
    <span class="n">n_y</span> <span class="o">=</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c"># n_y : output size</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c"># To keep track of the cost</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  <span class="c"># to mark the start of the training</span>

    <span class="c"># Create Placeholders of shape (n_x, n_y)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">create_placeholders</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
    <span class="n">keep_prob1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'keep_prob1'</span><span class="p">)</span>  <span class="c"># probability to keep a unit during dropout</span>
    <span class="n">keep_prob2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'keep_prob2'</span><span class="p">)</span>

    <span class="c"># Initialize parameters</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">()</span>

    <span class="c"># Forward propagation</span>
    <span class="n">Z4</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">)</span>

    <span class="c"># Cost function</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">Z4</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">regularizers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">'W3'</span><span class="p">])</span> \
                   <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">'W4'</span><span class="p">])</span>  <span class="c"># add regularization term</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c"># regularization constant</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">cost</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">regularizers</span><span class="p">)</span>  <span class="c"># cost with regularization</span>

    <span class="c"># Backpropagation: Define the tensorflow AdamOptimizer.</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="c"># Initialize all the variables</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="c"># Start the session to compute the tensorflow graph</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

        <span class="c"># Run the initialization</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

        <span class="c"># Do the training loop</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>

            <span class="n">epoch_cost</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c"># Defines a cost related to an epoch</span>
            <span class="n">num_minibatches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">m</span> <span class="o">/</span> <span class="n">minibatch_size</span><span class="p">)</span>  <span class="c"># number of minibatches of size minibatch_size in the train set</span>
            <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">minibatches</span> <span class="o">=</span> <span class="n">random_mini_batches</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">minibatch</span> <span class="ow">in</span> <span class="n">minibatches</span><span class="p">:</span>
                <span class="c"># Select a minibatch</span>
                <span class="p">(</span><span class="n">minibatch_X</span><span class="p">,</span> <span class="n">minibatch_Y</span><span class="p">)</span> <span class="o">=</span> <span class="n">minibatch</span>

                <span class="c"># Run the session to execute the "optimizer" and the "cost"</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">minibatch_cost</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">cost</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">minibatch_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">minibatch_Y</span><span class="p">,</span>
                                                                           <span class="n">keep_prob1</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
                <span class="n">epoch_cost</span> <span class="o">+=</span> <span class="n">minibatch_cost</span> <span class="o">/</span> <span class="n">num_minibatches</span>

            <span class="c"># Print the cost every epoch</span>
            <span class="k">if</span> <span class="n">print_cost</span> <span class="o">==</span> <span class="bp">True</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Cost after epoch {}: {:f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">epoch_cost</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">print_cost</span> <span class="o">==</span> <span class="bp">True</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_cost</span><span class="p">)</span>

        <span class="c"># lets save the parameters in a variable</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Parameters have been trained!"</span><span class="p">)</span>

        <span class="c"># Calculate the correct predictions</span>
        <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Z4</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
        <span class="c"># Calculate accuracy on the test set</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="s">"float"</span><span class="p">))</span>

        <span class="n">train_cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">X</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
        <span class="n">test_cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">X</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">keep_prob1</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">keep_prob2</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>

        <span class="k">print</span><span class="p">(</span><span class="s">'Finished training in </span><span class="si">%</span><span class="s">s s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Train Cost:"</span><span class="p">,</span> <span class="n">train_cost</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Test Cost:"</span><span class="p">,</span> <span class="n">test_cost</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Train Accuracy:"</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Test Accuracy:"</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">)</span>

        <span class="c"># plot the cost</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">costs</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'cost'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'iterations (per fives)'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Learning rate = {}, beta = {},</span><span class="se">\n</span><span class="s">"</span>
                  <span class="s">"test cost = {:.6f}, test accuracy = {:.6f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">test_cost</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">))</span>
        <span class="k">global</span> <span class="n">filename</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">timestr</span> <span class="o">+</span> <span class="s">'_NN4Lstage2_lr_{}_beta_{}_cost_{:.2f}-{:.2f}_acc_{:.2f}-{:.2f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">train_cost</span><span class="p">,</span> <span class="n">test_cost</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">dirname</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div></div>

<p>Note that the model function will return the learned parameters from the network and additionally will plot the cost after each epoch. The plot is also saved as a file that includes the timestamp as well as the learning rate, beta, cost and accuracy information for this particular run.</p>

<p>Now it’s time to train the model using the train and validation data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># train the model and get learned parameters</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cost after epoch 0: 6.607861
Cost after epoch 100: 1.389869
Cost after epoch 200: 0.988806
Cost after epoch 300: 0.882713
Cost after epoch 400: 0.833693
Cost after epoch 500: 0.811457
Cost after epoch 600: 0.793379
Cost after epoch 700: 0.773927
Cost after epoch 800: 0.762247
Cost after epoch 900: 0.767449
Parameters have been trained!
Finished training in 498.4203100204468 s
Train Cost: 0.665462
Test Cost: 1.74987
Train Accuracy: 0.979292
Test Accuracy: 0.643609
</code></pre></div></div>

<p><img src="/images/personalized-medicine_30_1.png" alt="png" /></p>

<p>From my validation results we can observe that the network learned nicely. However, the final cost of the training data was 0.665462, where as the validation data had a final cost of 1.74987. This is a large difference and an indication that the model is overfitting. Moreover the accuracy (defined here as the fraction of correct predictions) is very high (97.9%) for the training data and only 64.3% for the validation set. Another indication that the model is overfitting even though I have used both dropout and L2 regularization to counteract this.</p>

<h1 id="make-predictions">Make predictions</h1>

<p>We use the learned parameteres to make a prediction on the test data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># use learned parameters to make prediction on test data</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s look at an example of a prediction. As we can see below, the prediction consists of the probabilities of the entry belongin to each of the nine different categories (this was the format needed for this competition).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prediction</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 0.36503336,  0.21219006,  0.01297534,  0.14676626,  0.08375936,
        0.09217557,  0.02737238,  0.03150512,  0.02822249], dtype=float32)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prediction</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(9, 986)
</code></pre></div></div>

<p>All we have to do now is create a submission .csv file to save our prediction results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create submission file</span>
<span class="n">submission</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">prediction</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">submission</span><span class="p">[</span><span class="s">'id'</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_index</span>
<span class="n">submission</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'class1'</span><span class="p">,</span> <span class="s">'class2'</span><span class="p">,</span> <span class="s">'class3'</span><span class="p">,</span> <span class="s">'class4'</span><span class="p">,</span> <span class="s">'class5'</span><span class="p">,</span> <span class="s">'class6'</span><span class="p">,</span> <span class="s">'class7'</span><span class="p">,</span> <span class="s">'class8'</span><span class="p">,</span> <span class="s">'class9'</span><span class="p">,</span> <span class="s">'id'</span><span class="p">]</span>
<span class="n">submission</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">dirname</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="results-interpretation">Results interpretation</h2>

<p>Using this neural network model, my submission to Kaggle yielded following results:</p>

<ul>
  <li>Public score (based on a portion of the test data by Kaggle to provide an indication of performance during the competition): Loss = 1.69148</li>
  <li>Private score (based on a different portion of the test data by Kaggle to provide the final score at the end of the competition): Loss = 2.74500</li>
</ul>

<p>The discrepancy between these two scores further shows that overfitting is an issue in working with this data in a neural network model. My model could benefit from increasing the training data and a higher regularization.</p>

  </div></div>

		</section>
	</main></body>
</html>
